{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217b0b83",
   "metadata": {},
   "source": [
    "# DS 3001 Project Part 2: \n",
    "## Pre-Analysis Plan - Fraud Detection Analysis\n",
    "#### Goal: Analyze transaction-level data in order to understand the patterns that distinguish fraudulent transactions from legitimate ones and to build a model that can accurately predict fraud before it occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e998ecb",
   "metadata": {},
   "source": [
    "##### Research Question\n",
    "The central research question guiding this study is: Can transaction data accurately distinguish fraudulent transactions from legitimate ones and what patterns direct this distinction? This project aims to develop predictive models that accurately identify fraudulent transactions based on the characteristics of the transaction, user, merchant, timestamp, and device information. By analyzing these features, the study seeks to uncover underlying patterns that differentiate fraudulent behavior from normal transaction activity. Understanding these distinctions not only assists in the construction of effective fraud detection systems but also provides valuable insights into risk factors associated with transaction fraud.\n",
    "\n",
    "##### Unit of Observation\n",
    "In this analysis, the unit of observation is a single transaction. Each record in the dataset represents an individual transaction, capturing a wide array of attributes including the transaction amount, the type of merchant involved, details about the user initiating the transaction, the payment card and authentication methods used, and the device type employed. Additionally, each transaction is labeled as either fraudulent or legitimate, making the dataset well-suited for binary classification.\n",
    "\n",
    "##### Type of Learning Problem\n",
    "This project constitutes a supervised learning task. Specifically, it is a binary classification problem, wherein the goal is to assign each transaction to one of two possible classes: fraudulent or non-fraudulent (1 or 0 respectively). Given the binary nature of the outcome variable (fraud_label), classification algorithms are the appropriate choice for modeling. The objective is to build predictive models that can learn from historical transaction data and generalize effectively to new, unseen transactions.\n",
    "\n",
    "##### Planned Models and Algorithms\n",
    "A variety of modeling techniques will be employed to approach this classification problem. The initial model will be a logistic regression, serving as a baseline due to its interpretability and computational efficiency. Logistic regression offers a straightforward probabilistic framework for binary outcomes and will provide a benchmark against which more complex models can be evaluated.\n",
    "\n",
    "Following the baseline model, I will implement a decision tree classifier. Decision trees are particularly useful for their ability to model non-linear interactions between variables and for their inherent interpretability through visualization of decision rules. Building on this, I will use an ensemble approach using random forests, which aggregate multiple decision trees to reduce variance and improve predictive stability.\n",
    "\n",
    "Model training will involve hyperparameter tuning via cross-validation to optimize performance while minimizing overfitting. Stratified sampling will be used to preserve the proportion of fraudulent and legitimate transactions across training and testing datasets.\n",
    "\n",
    "##### Defining Success\n",
    "Success in this project will primarily be evaluated using the F1 score, which balances the trade-off between precision and recall. In fraud detection, it is particularly important to optimize both metrics because false positives (flagging legitimate transactions as fraud) can harm customer experience, while false negatives (missing actual fraud) can lead to financial losses. Thus, while overall accuracy is important, the F1 score provides a more informative metric in the presence of class imbalance.\n",
    "\n",
    "Secondary metrics will include precision, recall, accuracy, and the area under the receiver operating characteristic curve (ROC AUC). Confusion matrices will be generated to provide a detailed understanding of model performance across both classes. A successful model will achieve a high F1 score and demonstrate strong discrimination between fraudulent and legitimate transactions in out-of-sample data.\n",
    "\n",
    "##### Anticipated Challenges and Contingency Plans\n",
    "Several challenges are anticipated in this analysis. First, there is significant class imbalance, as fraudulent transactions are relatively rare compared to legitimate transactions. This imbalance can bias models toward the majority class, leading to poor detection of fraud. To address this, I will explore methods such as oversampling and class weighting during model training to ensure that the minority class is adequately represented.\n",
    "\n",
    "Second, given that the dataset is synthetic, there is a risk that models trained on it might overfit to artificial patterns that do not generalize to real-world transaction data. Careful validation techniques, including cross-validation and performance assessment on a hold-out test set, will be employed to monitor and mitigate overfitting.\n",
    "\n",
    "Third, feature redundancy and irrelevance could pose problems. Some features may not contribute meaningfully to the prediction task or may be highly correlated with others, introducing noise. Feature selection techniques and regularization will be considered to improve model robustness.\n",
    "\n",
    "If the original modeling approach fails to produce satisfactory results, I will reconsider feature engineering strategies or evaluate the inclusion of interaction terms and non-linear transformations of variables.\n",
    "\n",
    "##### Feature Engineering Strategy\n",
    "The dataset contains a combination of categorical and numerical variables. Categorical features such as Transaction_Type, Merchant_Category, Card_Type, and Authentication_Method will be one-hot encoded to enable their use in machine learning models. Numerical features, notably Amount, may require scaling for algorithms sensitive to feature magnitudes. Missing values will be examined and handled appropriately, either through imputation or by treating missingness itself as a potentially informative feature.\n",
    "\n",
    "##### Presentation of Results\n",
    "Results will be systematically presented to demonstrate the performance and limitations of the models. For each model, I will present a confusion matrix, report classification metrics (F1 score, precision, recall, accuracy, and ROC AUC), and plot precision-recall curves and ROC curves. Feature importance visualizations, particularly from tree-based models, will provide insights into which variables are most influential in distinguishing fraudulent transactions.\n",
    "\n",
    "Additionally, model comparisons will be summarized in a comprehensive table highlighting the strengths and weaknesses of each approach. Any failure of a model to achieve the desired level of performance will be documented alongside an analysis of potential reasons and future steps for improvement."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
